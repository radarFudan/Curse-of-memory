[[36m2023-09-05 09:42:19,536[39m][[34m__main__[39m][[32mINFO[39m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[[36m2023-09-05 09:42:19,614[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[[36m2023-09-05 09:42:19,621[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - GPU available: True (cuda), used: True
[[36m2023-09-05 09:42:19,621[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - TPU available: False, using: 0 TPU cores
[[36m2023-09-05 09:42:19,621[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - IPU available: False, using: 0 IPUs
[[36m2023-09-05 09:42:19,622[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - HPU available: False, using: 0 HPUs
[[36m2023-09-05 09:42:19,622[39m][[34m__main__[39m][[32mINFO[39m] - Logging hyperparameters!
Running validation before training
LF_generate done
In lf datagen, input shape (153600, 100, 1)
In lf datagen, output shape (153600, 100, 1)
In lf_datamodule.py, self.rho_name pol
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
/home/shida/anaconda3/envs/lh/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[[36m2023-09-05 09:42:23,683[39m][[34msrc.models.recurrent.s4[39m][[32mINFO[39m] - S4: Initializing kernel to length 100
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m      Validate metric      [22mâ”ƒ[1m       DataLoader 0        [22mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚[36m         val/loss          [39mâ”‚[35m    15.279667854309082     [39mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[37mValidation[39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m25/25[39m [37m0:00:01 â€¢ 0:00:00[39m [37m91.22it/s
[?25h[[36m2023-09-05 09:42:24,226[39m][[34m__main__[39m][[32mINFO[39m] - Starting training!
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
LF_generate done
In lf datagen, input shape (153600, 100, 1)
In lf datagen, output shape (153600, 100, 1)
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m    [22mâ”ƒ[1m Name                  [22mâ”ƒ[1m Type          [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net                   â”‚ S4Block       â”‚ 20.7 K â”‚
â”‚ 1  â”‚ net.layer             â”‚ FFTConv       â”‚ 16.5 K â”‚
â”‚ 2  â”‚ net.layer.activation  â”‚ GELU          â”‚      0 â”‚
â”‚ 3  â”‚ net.layer.kernel      â”‚ SSMKernelDPLR â”‚ 16.4 K â”‚
â”‚ 4  â”‚ net.layer.drop        â”‚ Identity      â”‚      0 â”‚
â”‚ 5  â”‚ net.layer.drop_kernel â”‚ Identity      â”‚      0 â”‚
â”‚ 6  â”‚ net.mult_activation   â”‚ Identity      â”‚      0 â”‚
â”‚ 7  â”‚ net.drop              â”‚ Identity      â”‚      0 â”‚
â”‚ 8  â”‚ net.output_linear     â”‚ Sequential    â”‚  4.2 K â”‚
â”‚ 9  â”‚ net.output_linear.0   â”‚ Linear        â”‚  4.2 K â”‚
â”‚ 10 â”‚ net.output_linear.1   â”‚ Hardtanh      â”‚      0 â”‚
â”‚ 11 â”‚ encoder               â”‚ Linear        â”‚    128 â”‚
â”‚ 12 â”‚ encoder.U             â”‚ Linear        â”‚    128 â”‚
â”‚ 13 â”‚ encoder.activation    â”‚ Identity      â”‚      0 â”‚
â”‚ 14 â”‚ decoder               â”‚ Linear        â”‚     65 â”‚
â”‚ 15 â”‚ decoder.U             â”‚ Linear        â”‚     65 â”‚
â”‚ 16 â”‚ decoder.activation    â”‚ Identity      â”‚      0 â”‚
â”‚ 17 â”‚ train_loss            â”‚ MeanMetric    â”‚      0 â”‚
â”‚ 18 â”‚ val_loss              â”‚ MeanMetric    â”‚      0 â”‚
â”‚ 19 â”‚ test_loss             â”‚ MeanMetric    â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 20.9 K
[1mNon-trainable params[22m: 0
[1mTotal params[22m: 20.9 K
[1mTotal estimated model params size (MB)[22m: 0
/home/shida/anaconda3/envs/lh/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck.
Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(



Epoch 1/999 [35mâ”â”â”â”â•¸[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m30/250[39m [37m0:00:00 â€¢ 0:00:06[39m [37m37.91it/s[39m [37mv_num: zfk9 train/loss: 0.209 val/loss: 0.481 




Epoch 2/999 [35mâ”â”â”â”â”â”â”â•¸[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m48/250[39m [37m0:00:01 â€¢ 0:00:06[39m [37m40.16it/s[39m [37mv_num: zfk9 train/loss: 0.08 val/loss: 0.126 



Epoch 3/999 [35mâ”â”â•¸[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m16/250[39m [37m0:00:00 â€¢ 0:00:07[39m [37m38.62it/s[39m [37mv_num: zfk9 train/loss: 0.055 val/loss: 0.021 




Epoch 4/999 [35mâ”â”â”â”â”â”â”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m51/250[39m [37m0:00:01 â€¢ 0:00:05[39m [37m40.19it/s[39m [37mv_num: zfk9 train/loss: 0.006 val/loss: 0.018 







Epoch 6/999 [35mâ”â”â”â”â”â”â”â”â•¸[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m55/250[39m [37m0:00:01 â€¢ 0:00:05[39m [37m39.60it/s[39m [37mv_num: zfk9 train/loss: 0.005 val/loss: 0.01 


Epoch 6/999 [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m250/250[39m [37m0:00:06 â€¢ 0:00:00[39m [37m38.63it/s[39m [37mv_num: zfk9 train/loss: 0.005 val/loss: 0.004 
[?25h[[36m2023-09-05 09:43:15,692[39m][[34m__main__[39m][[32mINFO[39m] - Starting testing!
Stopping threshold reached: val/loss = 0.00435843039304018 < 0.009552148170769215. Signaling Trainer to stop.
LF_generate done
In lf datagen, input shape (153600, 100, 1)
In lf datagen, output shape (153600, 100, 1)
[[36m2023-09-05 09:43:16,930[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Restoring states from the checkpoint path at /home/shida/Figure_1/logs/LF_hardtanh_s4_pol/runs/20230905/094212/checkpoints/epoch_006.ckpt
[[36m2023-09-05 09:43:16,949[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Loaded model weights from the checkpoint at /home/shida/Figure_1/logs/LF_hardtanh_s4_pol/runs/20230905/094212/checkpoints/epoch_006.ckpt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m      Validate metric      [22mâ”ƒ[1m       DataLoader 0        [22mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚[36m         val/loss          [39mâ”‚[35m    0.00435843039304018    [39mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[37mValidation[39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m25/25[39m [37m0:00:00 â€¢ 0:00:00[39m [37m90.14it/s
[?25h
[?25hLF_generate done
In lf datagen, input shape (153600, 100, 1)
In lf datagen, output shape (153600, 100, 1)
[[36m2023-09-05 09:43:18,782[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Restoring states from the checkpoint path at /home/shida/Figure_1/logs/LF_hardtanh_s4_pol/runs/20230905/094212/checkpoints/epoch_006.ckpt
[[36m2023-09-05 09:43:18,802[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Loaded model weights from the checkpoint at /home/shida/Figure_1/logs/LF_hardtanh_s4_pol/runs/20230905/094212/checkpoints/epoch_006.ckpt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m        Test metric        [22mâ”ƒ[1m       DataLoader 0        [22mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚[36m         test/loss         [39mâ”‚[35m   0.004405083600431681    [39mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[37mTesting[39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m25/25[39m [37m0:00:00 â€¢ 0:00:00[39m [37m89.88it/s
[?25h[[36m2023-09-05 09:43:19,392[39m][[34m__main__[39m][[32mINFO[39m] - Best ckpt path: /home/shida/Figure_1/logs/LF_hardtanh_s4_pol/runs/20230905/094212/checkpoints/epoch_006.ckpt
[[36m2023-09-05 09:43:19,394[39m][[34msrc.utils.utils[39m][[32mINFO[39m] - Output dir: /home/shida/Figure_1/logs/LF_hardtanh_s4_pol/runs/20230905/094212
[[36m2023-09-05 09:43:19,394[39m][[34msrc.utils.utils[39m][[32mINFO[39m] - Closing wandb!
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
/home/shida/anaconda3/envs/lh/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(