# defaults:
#   - /model/optimizer/adamW.yaml
#   - /model/scheduler/reduceLROnPlateau.yaml

_target_: src.models.lf_module.LFLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

net:
  _target_: src.models.recurrent.s4.S4Block
  rec1_size: 1
  transposed: False # x: (B H L) if transposed else (B L H)
  return_sequences: True
  final_act: "linear" # Otherwise will apply a glu to the final output
  deterministic: True

encoder:
  _target_: src.models.encoders.linear.Linear
  in_size: 1
  out_size: ${model.net.rec1_size}
decoder:
  _target_: src.models.decoders.linear.Linear
  in_size: ${model.net.rec1_size}
  out_size: 1
